library(dplyr)
library(r2symbols)
library(tidyr)
library(formatR)
#| echo: true
#| label: fig-scatter
#| fig-cap: "This is a scatter plot of the Collider data in the X1 and X2 feature space, with points colour-coded by true particle class, illustrating non-linear boundaries that motivate using a neural network for classification."
dat$response <- apply(dat[, c("Y1", "Y2", "Y3")], 1, function (x){
if (x[1] == 1) return("code-Alpha")
if (x[2] == 1) return("code-Beta")
if (x[3] == 1) return("code-Rho")
})
#1:1 aspect ratio
ggplot(dat, aes(x=X1, y=X2, color=response))+
geom_point(size=2)+
coord_fixed()+
labs(title = "Scatterplot of particles in feature space",
x = "First coordinate (X1)", y = "Second coordinate (X2)",
color = "Particle type")+ theme_minimal()
#| echo: true
softmax <- function(Z)
{
Z_shift <- Z - matrix(apply(Z, 2, max), nrow = 3, ncol = ncol(Z),
byrow = TRUE)
#Subtract the column max for numerical stability (to avoid computational overflow when exponentiating)
expZ    <- exp(Z_shift)
denom   <- matrix(colSums(expZ),
nrow = 3, ncol = ncol(Z), byrow =TRUE)
# column-wise sums...sum across 3 classes
# convert to matrix for conformability
expZ / denom
}
#| echo: true
g <- function(Yhat, Y, eps = 1e-15) {
# Yhat, Y : N × q matrices   (rows = observations, columns = classes)
N <- nrow(Y)
-sum( Y * log( pmax(Yhat, eps) ) ) / N
# pmax() replaces any element of Yhat that is smaller than eps with eps   #ensures no value passed to log() is <= zero
}
#| echo: true
# X: input matrix (N x p)
# Y: output matrix (N x q)
# theta: parameter vector with all weights and biases
# m: number of nodes on hidden layer
# v: regularisation parameter
af_forward <- function(X, Y, theta, m, v)
{
N <- nrow(X)
p <- ncol(X)
q <- ncol(Y)
# Populate weight-matrix and bias vectors by unpacking theta:
index <- 1:(2*(p^2)) #W1 : p(p+p)
W1 <- matrix(theta[index], nrow=p)
index <- max(index)+1:(2*p) #b1 : (p+p)
b1 <- theta[index]
index <- max(index)+1:((2*p)*m) #W2 : (p+p)*m
W2 <- matrix(theta[index], nrow=2*p)
index <- max(index)+1:m #b2 : m
b2 <- theta[index]
index <- max(index)+1:(m*m) #W3 : (m*m)
W3 <- matrix(theta[index], nrow=m)
index <- max(index)+1:m #b3 : m
b3 <- theta[index]
index <- max(index)+1:(m*q) #W4 : (m*q)
W4 <- matrix(theta[index], nrow=m)
index <- max(index)+1:q #b4 : q
b4 <- theta[index]
#forward propagation
H1 <- tanh( X  %*% W1 + matrix(b1, N, 2*p, TRUE) ) # aug-layer output
H2 <- tanh( H1 %*% W2 + matrix(b2, N, m, TRUE) ) # 1st hidden layer output
H3 <- tanh(H2 %*% W3 + matrix(b3, N, m, TRUE)) # 2nd hidden layer output
Z <- H3 %*% W4 + matrix(b4, N, q, TRUE) # final layer to get logits
# apply softmax across logits
P_3byN <- softmax(t(Z))
# temporarily transpose because softmax expects input where columns are different samples
probs   <- t(P_3byN)
#losses & objective
loss <- g(probs, Y) # cross-entropy
obj  <- loss + (v / 2) * sum(theta^2) # L2 regularisation
list(probs = probs, loss = loss, obj = obj)
}
#| echo: true
#| label: fig-valid
#| fig-cap: "Validation cross-entropy loss on the held-out set plotted against the L2-regularization parameter v(log-scaled) for an AF-network with m=4. The red dashed line marks the chosen optimal v. We use a logarithmic scale because v ranges over several orders of magnitude; the log‐scale stretches out the small v region so that the classic U‐shaped trade‐off between under‐ and over‐regularization becomes visible."
set.seed(2025)
m <- 4
# Step 1: Split the data into training and validation sets (80%/20%)
n <- nrow(dat)
train_size <- floor(0.8 * n)
train_indices <- sample(1:n, train_size)
train_data <- dat[train_indices, ]
valid_data <- dat[-train_indices, ]
# Step 2: Prepare the training and validation datasets
X_train <- as.matrix(train_data[, 1:3])  # Input features
Y_train <- as.matrix(train_data[, 4:6])  # Response variables (one-hot encoded)
X_valid <- as.matrix(valid_data[, 1:3])  # Input features
Y_valid <- as.matrix(valid_data[, 4:6])  # Response variables (one-hot encoded)
# Step 3: Define the objective function with regularization
v=0.01
obj_pen <- function(theta)
{
result <- af_forward(X_train, Y_train, theta, m, v)
return(result$obj)
}
# Initial random theta
p <- ncol(X_train)
q <- ncol(Y_train)
m <- 4
npars  <- 2*p^2 + 2*p + 2*p*m + 2*m + m^2 + m*q + q
theta_rand = runif(npars,-1,1)
res_opt = nlm(obj_pen,theta_rand,iterlim = 1000)
# Step 4: Grid search over regularization parameter nu
n_v <- 15
validation_errors = rep(NA,n_v)
v_values <- exp(seq(-6, 2, length.out = n_v))
#validation_errors <- numeric(length(v_values))
for (i in 1:n_v)
{
v <- v_values[i]
res_opt = nlm(obj_pen,theta_rand,iterlim = 1000)
res_val = af_forward(X_valid,Y_valid,res_opt$estimate,m,0)
validation_errors[i] = res_val$obj
}
best_i  <- which.min(validation_errors)
best_v <- v_values[best_i]
# Step 5: Plot validation error vs v
plot(v_values, validation_errors, type = 'b', log = "x", pch = 16,
xlab = expression(nu),
ylab = "Validation Cross-Entropy Loss",
main = "Validation Loss vs Regularization",
asp = 1)
abline(v = best_v, col = "red", lty = 2)
points(best_v, validation_errors[best_i], col = "red", pch = 16)
legend("topright",
legend = paste0("Chosen v=", signif(best_v,3)),
col    = "red", lty = 2, pch = 16, bty = "n")
plot(v_values, validation_errors, type = 'b', pch = 16,
xlab = expression(nu),
ylab = "Validation Cross-Entropy Loss",
main = "Validation Loss vs Regularization",
asp = 1)
View(X_valid)
View(Y_train)
#| echo: true
#| label: fig-valid
#| fig-cap: "Validation cross-entropy loss on the held-out set plotted against the L2-regularization parameter v(log-scaled) for an AF-network with m=4. The red dashed line marks the chosen optimal v. We use a logarithmic scale because v ranges over several orders of magnitude; the log‐scale stretches out the small v region so that the classic U‐shaped trade‐off between under‐ and over‐regularization becomes visible."
set.seed(2025)
m <- 4
# Step 1: Split the data into training and validation sets (80%/20%)
n <- nrow(dat)
train_size <- floor(0.8 * n)
train_indices <- sample(1:n, train_size)
train_data <- dat[train_indices, ]
valid_data <- dat[-train_indices, ]
# Step 2: Prepare the training and validation datasets
X_train <- as.matrix(train_data[, 1:3])  # Input features
Y_train <- as.matrix(train_data[, 4:6])  # Response variables (one-hot encoded)
X_valid <- as.matrix(valid_data[, 1:3])  # Input features
Y_valid <- as.matrix(valid_data[, 4:6])  # Response variables (one-hot encoded)
# Step 3: Define the objective function with regularization
v=0.01
obj_pen <- function(theta)
{
result <- af_forward(X_train, Y_train, theta, m, v)
return(result$obj)
}
# Initial random theta
p <- ncol(X_train)
q <- ncol(Y_train)
m <- 4
npars  <- 2*p^2 + 2*p + 2*p*m + 2*m + m^2 + m*q + q
theta_rand = runif(npars,-1,1)
res_opt = nlm(obj_pen,theta_rand,iterlim = 1000)
# Step 4: Grid search over regularization parameter nu
n_v <- 15
validation_errors = rep(NA,n_v)
v_values <- exp(seq(-4, 2, length.out = n_v))
#validation_errors <- numeric(length(v_values))
for (i in 1:n_v)
{
v <- v_values[i]
res_opt = nlm(obj_pen,theta_rand,iterlim = 1000)
res_val = af_forward(X_valid,Y_valid,res_opt$estimate,m,0)
validation_errors[i] = res_val$obj
}
best_i  <- which.min(validation_errors)
best_v <- v_values[best_i]
# Step 5: Plot validation error vs v
plot(v_values, validation_errors, type = 'b', pch = 16,
xlab = expression(nu),
ylab = "Validation Cross-Entropy Loss",
main = "Validation Loss vs Regularization",
asp = 1)
abline(v = best_v, col = "red", lty = 2)
points(best_v, validation_errors[best_i], col = "red", pch = 16)
legend("topright",
legend = paste0("Chosen v=", signif(best_v,3)),
col    = "red", lty = 2, pch = 16, bty = "n")
#| echo: true
#| label: fig-valid
#| fig-cap: "Validation cross-entropy loss on the held-out set plotted against the L2-regularization parameter v(log-scaled) for an AF-network with m=4. The red dashed line marks the chosen optimal v. We use a logarithmic scale because v ranges over several orders of magnitude; the log‐scale stretches out the small v region so that the classic U‐shaped trade‐off between under‐ and over‐regularization becomes visible."
set.seed(2025)
m <- 4
# Step 1: Split the data into training and validation sets (80%/20%)
n <- nrow(dat)
train_size <- floor(0.8 * n)
train_indices <- sample(1:n, train_size)
train_data <- dat[train_indices, ]
valid_data <- dat[-train_indices, ]
# Step 2: Prepare the training and validation datasets
X_train <- as.matrix(train_data[, 1:3])  # Input features
Y_train <- as.matrix(train_data[, 4:6])  # Response variables (one-hot encoded)
X_valid <- as.matrix(valid_data[, 1:3])  # Input features
Y_valid <- as.matrix(valid_data[, 4:6])  # Response variables (one-hot encoded)
# Step 3: Define the objective function with regularization
v=0.01
obj_pen <- function(theta)
{
result <- af_forward(X_train, Y_train, theta, m, v)
return(result$obj)
}
# Initial random theta
p <- ncol(X_train)
q <- ncol(Y_train)
m <- 4
npars  <- 2*p^2 + 2*p + 2*p*m + 2*m + m^2 + m*q + q
theta_rand = runif(npars,-1,1)
res_opt = nlm(obj_pen,theta_rand,iterlim = 1000)
# Step 4: Grid search over regularization parameter nu
n_v <- 15
validation_errors = rep(NA,n_v)
v_values <- exp(seq(-8, 3, length.out = n_v))
#validation_errors <- numeric(length(v_values))
for (i in 1:n_v)
{
v <- v_values[i]
res_opt = nlm(obj_pen,theta_rand,iterlim = 1000)
res_val = af_forward(X_valid,Y_valid,res_opt$estimate,m,0)
validation_errors[i] = res_val$obj
}
best_i  <- which.min(validation_errors)
best_v <- v_values[best_i]
# Step 5: Plot validation error vs v
plot(v_values, validation_errors, type = 'b', pch = 16,
xlab = expression(nu),
ylab = "Validation Cross-Entropy Loss",
main = "Validation Loss vs Regularization",
asp = 1)
abline(v = best_v, col = "red", lty = 2)
points(best_v, validation_errors[best_i], col = "red", pch = 16)
legend("topright",
legend = paste0("Chosen v=", signif(best_v,3)),
col    = "red", lty = 2, pch = 16, bty = "n")
plot(v_values, validation_errors, type = 'b', log="x", pch = 16,
xlab = expression(nu),
ylab = "Validation Cross-Entropy Loss",
main = "Validation Loss vs Regularization",
asp = 1)
abline(v = best_v, col = "red", lty = 2)
plot(v_values, validation_errors, type = 'b', log="x", pch = 16,
xlab = expression(nu),
ylab = "Validation Cross-Entropy Loss",
main = "Validation Loss vs Regularization",
asp = 1)
abline(v = best_v, col = "red", lty = 2)
points(best_v, validation_errors[best_i], col = "red", pch = 16)
legend("topright",
legend = paste0("Chosen v=", signif(best_v,3)),
col    = "red", lty = 2, pch = 16, bty = "n")
#####new year code-----------
# 1. Read in the data
raw <- read_excel(
"Peregrine ringing data sightings_1989-2024_13042025.xlsx",
sheet = "Re-sightings - annual",
skip  = 3
)
library(readxl)
#####new year code-----------
# 1. Read in the data
raw <- read_excel(
"Peregrine ringing data sightings_1989-2024_13042025.xlsx",
sheet = "Re-sightings - annual",
skip  = 3
)
#####------------------------
raw <- read_excel(
"Peregrine ringing data sightings_1989-2024_13042025.xlsx",
sheet = "Re-sightings - annual",
skip = 3 # skip metadata rows
)
setwd("~/Documents/GitHub/research-project")
#####new year code-----------
# 1. Read in the data
raw <- read_excel(
"Peregrine ringing data sightings_1989-2024_13042025.xlsx",
sheet = "Re-sightings - annual",
skip  = 3
)
View(raw)
# 2. Tidy & compute “capture‐year” (Aug→Jul)
dat <- raw %>%
rename(
ring      = `ring number`,
date_seen = `date sighted on territory`
) %>%
mutate(
date_seen = as.Date(date_seen),
# if month ≥ 8 (Aug–Dec), cap_year = calendar year;
# else (Jan–Jul), assign to previous year
cap_year = if_else(
month(date_seen) >= 8,
year(date_seen),
year(date_seen) - 1L
)
) %>%
select(ring, cap_year) %>%
distinct() %>%
mutate(obs = 1L)
library(lubridate)
library(dplyr)
library(tidyr)
# 2. Tidy & compute “capture‐year” (Aug→Jul)
dat <- raw %>%
rename(
ring      = `ring number`,
date_seen = `date sighted on territory`
) %>%
mutate(
date_seen = as.Date(date_seen),
# if month ≥ 8 (Aug–Dec), cap_year = calendar year;
# else (Jan–Jul), assign to previous year
cap_year = if_else(
month(date_seen) >= 8,
year(date_seen),
year(date_seen) - 1L
)
) %>%
select(ring, cap_year) %>%
distinct() %>%
mutate(obs = 1L)
View(dat)
# 3. Expand to every bird × every capture‐year and fill zeros
years <- min(dat$cap_year):max(dat$cap_year)
min(dat$cap_year)
max(dat$cap_year)
# 3. Expand to every bird × every capture‐year and fill zeros
years <- seq(min(dat$cap_year, na.rm = TRUE), max(dat$cap_year, na.rm = TRUE))
full <- expand.grid(
ring     = unique(dat$ring),
cap_year = years
) %>%
left_join(dat, by = c("ring","cap_year")) %>%
mutate(obs = replace_na(obs, 0L))
View(raw)
View(full)
# 4. Pivot to wide encounter‐history matrix
enc_hist <- full %>%
arrange(ring, cap_year) %>%
pivot_wider(
names_from  = cap_year,
values_from = obs,
names_prefix = "yr",
values_fill  = 0L
)
ch_matrix <- as.matrix(enc_hist[,-1])
rownames(ch_matrix) <- enc_hist$ring
View(ch_matrix)
# 5. (Optional) write the .inp capture‐history + freq
cap_hist_inp <- enc_hist %>%
mutate(
caphist_str = apply(select(., starts_with("yr")), 1, paste0, collapse = "")
) %>%
select(caphist_str) %>%
mutate(freq = 1)
View(cap_hist_inp)
write.table(
cap_hist_inp,
"peregrine_data.inp",
quote     = FALSE,
row.names = FALSE,
col.names = FALSE,
sep       = " "
)
write.table(
cap_hist_inp,
"peregrine_data_newyear.inp",
quote     = FALSE,
row.names = FALSE,
col.names = FALSE,
sep       = " "
)
#####------------------------
raw <- read_excel(
"Peregrine ringing data sightings_1989-2024_13042025.xlsx",
sheet = "Re-sightings - annual",
skip = 3 # skip metadata rows
)
# 3. Tidy up: rename the ring ID and the year‐sighted columns,
#    and keep just those plus make an “obs” flag = 1
dat <- raw %>%
rename(
ring      = `ring number`,
date_seen = `date sighted on territory`
) %>%
# turn the POSIX date into an integer year
mutate(year_seen = year(date_seen)) %>%
select(ring, year_seen) %>%
distinct() %>%              # one (ring, year) per bird-year
mutate(obs = 1L)            # 1 = seen
# 4. Build a full table of (ring × all years in your study) and join...so even if a bird wasn't seen in a certain year that (ring, year) pair still exists...to be filled with 0s later
years <- 1990:2019
all_combos <- expand.grid(
ring      = unique(dat$ring),
year_seen = years
)
full <- all_combos %>%
left_join(dat, by = c("ring", "year_seen")) %>%
mutate(obs = replace_na(obs, 0L)) # If a bird was not seen in a year, obs is set to 0 using replace_na.
enc_hist <- full %>%
arrange(ring, year_seen) %>%
pivot_wider(
names_from  = year_seen,
values_from = obs,
names_prefix = "yr",
values_fill  = 0L
)
# drop the ring column if you just want the matrix
ch_matrix <- as.matrix(enc_hist[,-1])
rownames(ch_matrix) <- enc_hist$ring
# Or, if you want a 0/1 string:
cap_hist <- enc_hist %>%
mutate(
caphist_str = apply(select(., starts_with("yr")), 1, paste0, collapse = "")
)
# Optional: add frequency column (if each is unique, all freq = 1)
cap_hist_inp <- cap_hist %>%
select(caphist_str) %>%
mutate(freq = 1)
# Write to .inp file (no header, space-separated)
write.table(
cap_hist_inp,
file = "peregrine_data.inp",
quote = FALSE,
row.names = FALSE,
col.names = FALSE,
sep = " "
)
View(raw)
min(raw$`date ringed`)
min(raw$`date sighted on territory`)
min(dat$cap_year, na.rm = TRUE)
#####new year code-----------
# 1. Read the raw data
raw <- read_excel(
"Peregrine ringing data sightings_1989-2024_13042025.xlsx",
sheet = "Re-sightings - annual",
skip  = 3
)
# 2. Compute an "Aug–Jul capture‐year" and flag sightings
dat <- raw %>%
rename(
ring      = `ring number`,
date_seen = `date sighted on territory`
) %>%
mutate(
date_seen = as.Date(date_seen),
# if month ≥ 8 (Aug–Dec) → same calendar year; else (Jan–Jul) → previous year
cap_year = if_else(
month(date_seen) >= 8,
year(date_seen),
year(date_seen) - 1L
)
) %>%
select(ring, cap_year) %>%
distinct() %>%
mutate(obs = 1L) %>%
# keep only your 30 seasons 1990–2019
filter(cap_year >= 1990, cap_year <= 2019)
# 3. Build every ring × Aug–Jul season from 1990 to 2019
years <- 1990:2019
all_combos <- expand.grid(
ring     = unique(dat$ring),
cap_year = years
)
full <- all_combos %>%
left_join(dat, by = c("ring","cap_year")) %>%
mutate(obs = replace_na(obs, 0L))
# 4. Pivot into your 0/1 encounter‐history (yr1990 … yr2019)
enc_hist <- full %>%
arrange(ring, cap_year) %>%
pivot_wider(
names_from   = cap_year,
values_from  = obs,
names_prefix = "yr",
values_fill  = 0L
)
# 5. (Optional) Get a matrix or .inp file exactly as before:
ch_matrix <- as.matrix(enc_hist[,-1])
rownames(ch_matrix) <- enc_hist$ring
# if you need the .inp format:
caphist_inp <- enc_hist %>%
mutate(caphist = apply(select(., starts_with("yr")), 1, paste0, collapse = "")) %>%
select(caphist) %>%
mutate(freq = 1)
write.table(
caphist_inp,
"peregrine_data_newyear.inp",
quote     = FALSE,
row.names = FALSE,
col.names = FALSE,
sep       = " "
)
