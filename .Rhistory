validation_errors <- numeric(length(v_values))
for (i in 1:length(v_values)) {
v <- v_values[i]
# Initial random theta
p <- ncol(X_train)
q <- ncol(Y_train)
m <- 4
npars  <- 2*p^2 + 2*p + 2*p*m + 2*m + m^2 + m*q + q
theta_rand <- runif(npars, -1, 1)
# Fit the model using optim() to minimize the objective function
fit <- nlm(theta_rand, objective_fn, X = X_train, Y = Y_train, m = 4,               v=v)
# Get the predicted probabilities for validation set
Yhat_valid <- af_forward(X_valid, Y_valid, fit$par, m = 4, v=v)$probs
# Compute the validation error
validation_errors[i] <- g(Yhat_valid, Y_valid)
}
for (i in 1:length(v_values)) {
v <- v_values[i]
# Initial random theta
p <- ncol(X_train)
q <- ncol(Y_train)
m <- 4
npars  <- 2*p^2 + 2*p + 2*p*m + 2*m + m^2 + m*q + q
theta_rand <- runif(npars, -1, 1)
# Fit the model using optim() to minimize the objective function
fit <-  nlm(obj_pen, theta_rand, iterlim = 1000)
# Get the predicted probabilities for validation set
Yhat_valid <- af_forward(X_valid, Y_valid, fit$par, m = 4, v=v)$probs
# Compute the validation error
validation_errors[i] <- g(Yhat_valid, Y_valid)
}
for (i in 1:length(v_values)) {
v <- v_values[i]
# Initial random theta
p <- ncol(X_train)
q <- ncol(Y_train)
m <- 4
npars  <- 2*p^2 + 2*p + 2*p*m + 2*m + m^2 + m*q + q
theta_rand <- runif(npars, -1, 1)
# Fit the model using optim() to minimize the objective function
fit <-  nlm(obj_pen, theta_rand, iterlim = 1000)
# Get the predicted probabilities for validation set
Yhat_valid <- af_forward(X_valid, Y_valid, fit$par)$probs
# Compute the validation error
validation_errors[i] <- g(Yhat_valid, Y_valid)
}
set.seed(2025)
# Step 1: Split the data into training and validation sets (80%/20%)
n <- nrow(dat)
train_size <- floor(0.8 * n)
train_indices <- sample(1:n, train_size)
train_data <- dat[train_indices, ]
valid_data <- dat[-train_indices, ]
# Step 2: Prepare the training and validation datasets
# Input features
X_train <- as.matrix(train_data[, 1:3])
# Response variables(one-hot encoded)
Y_train <- as.matrix(train_data[, 4:6])
# Input features
X_valid <- as.matrix(valid_data[, 1:3])
# Response variables(one-hot encoded)
Y_valid <- as.matrix(valid_data[, 4:6])
# Step 3: Define the objective function with regularization
objective_fn <- function(theta, X, Y, m, v) {
result <- af_forward(X, Y, theta, m, v)
return(result$obj)
}
# Step 4: Grid search over regularization parameter nu
v_values <- exp(seq(-6, 2, length.out = 15))
validation_errors <- numeric(length(v_values))
for (i in 1:length(v_values)) {
v <- v_values[i]
# Initial random theta
p <- ncol(X_train)
q <- ncol(Y_train)
m <- 4
npars  <- 2*p^2 + 2*p + 2*p*m + 2*m + m^2 + m*q + q
theta_rand <- runif(npars, -1, 1)
# Fit the model using optim() to minimize the objective function
fit <-  nlm(obj_pen, theta_rand, iterlim = 1000)
# Get the predicted probabilities for validation set
Yhat_valid <- af_forward(X_valid, Y_valid, fit$par)$probs
# Compute the validation error
validation_errors[i] <- g(Yhat_valid, Y_valid)
}
exp(-2)
set.seed(2025)
m <- 4
# Step 1: Split the data into training and validation sets (80%/20%)
n <- nrow(dat)
train_size <- floor(0.8 * n)
train_indices <- sample(1:n, train_size)
train_data <- dat[train_indices, ]
valid_data <- dat[-train_indices, ]
# Step 2: Prepare the training and validation datasets
X_train <- as.matrix(train_data[, 1:3])  # Input features
Y_train <- as.matrix(train_data[, 4:6])  # Response variables (one-hot encoded)
X_valid <- as.matrix(valid_data[, 1:3])  # Input features
Y_valid <- as.matrix(valid_data[, 4:6])  # Response variables (one-hot encoded)
# Step 3: Define the objective function with regularization
v=0.01
obj_pen() <- function(pars) {
result <- af_forward(X_train, Y_train, theta, m, v)
return(result$obj)
}
obj_pen <- function(pars) {
result <- af_forward(X_train, Y_train, theta, m, v)
return(result$obj)
}
theta_rand = runif(npars,-1,1)
obj_pen(theta_rand)
res_opt = nlm(obj_pen,theta_rand,iterlim = 1000)
# 1. Split 80/20 (with seed)
set.seed(2025)
N           <- nrow(dat)
train_idx   <- sample(1:N, 0.8 * N, replace = FALSE)
valid_idx   <- setdiff(1:N, train_idx)
X_train     <- as.matrix(dat[train_idx, 1:3])
Y_train     <- as.matrix(dat[train_idx, 4:6])
X_valid     <- as.matrix(dat[valid_idx, 1:3])
Y_valid     <- as.matrix(dat[valid_idx, 4:6])
# 2. Network dimensions & parameter count
m      <- 4
p      <- ncol(X_train)
q      <- ncol(Y_train)
npars  <- 2*p^2 + 2*p + 2*p*m + 2*m + m^2 + m*q + q
# 3. Initialise random θ (as in lecture)
theta_rand <- runif(npars, -1, 1)
# 4. Grid of regularisation strengths
n_nu   <- 15
nu_seq <- exp(seq(-8, 2, length.out = n_nu))
# 5. Loop: fit on training, evaluate on validation
Val_error <- numeric(n_nu)
for (i in seq_len(n_nu)) {
nu <- nu_seq[i]
# penalised objective on training set
obj_pen <- function(pars) {
af_forward(X_train, Y_train, pars, m, nu)$obj
}
# fit using nlm()
res_opt <- nlm(obj_pen, theta_rand, iterlim = 1000)
# record unpenalised cross‐entropy on validation set
Val_error[i] <- af_forward(
X_valid, Y_valid,
res_opt$estimate, m, 0
)$loss
cat("nu =", signif(nu,3),
"→ val loss =", signif(Val_error[i],4), "\n")
}
plot(nu_seq, Val_error, type = "b", pch = 16,
log  = "x",
xlab = expression(nu),
ylab = "Validation Cross‐Entropy Loss",
main = "Validation Loss vs Regularization (m = 4)")
# 6. Plot validation loss vs ν on log‐x
best_i  <- which.min(Val_error)
best_nu <- nu_seq[best_i]
abline(v = best_nu, col = "red", lty = 2)
points(best_nu, Val_error[best_i], col = "red", pch = 16)
legend("topright",
legend = paste0("Chosen ν = ", signif(best_nu,3)),
col    = "red", lty = 2, pch = 16, bty = "n")
#| echo: true
set.seed(2025)
m <- 4
# Step 1: Split the data into training and validation sets (80%/20%)
n <- nrow(dat)
train_size <- floor(0.8 * n)
train_indices <- sample(1:n, train_size)
train_data <- dat[train_indices, ]
valid_data <- dat[-train_indices, ]
# Step 2: Prepare the training and validation datasets
X_train <- as.matrix(train_data[, 1:3])  # Input features
Y_train <- as.matrix(train_data[, 4:6])  # Response variables (one-hot encoded)
X_valid <- as.matrix(valid_data[, 1:3])  # Input features
Y_valid <- as.matrix(valid_data[, 4:6])  # Response variables (one-hot encoded)
# Step 3: Define the objective function with regularization
v=0.01
obj_pen <- function(theta)
{
result <- af_forward(X_train, Y_train, theta, m, v)
return(result$obj)
}
# Initial random theta
p <- ncol(X_train)
q <- ncol(Y_train)
m <- 4
npars  <- 2*p^2 + 2*p + 2*p*m + 2*m + m^2 + m*q + q
theta_rand = runif(npars,-1,1)
obj_pen(theta_rand)
res_opt = nlm(obj_pen,theta_rand,iterlim = 1000)
#| echo: true
set.seed(2025)
m <- 4
# Step 1: Split the data into training and validation sets (80%/20%)
n <- nrow(dat)
train_size <- floor(0.8 * n)
train_indices <- sample(1:n, train_size)
train_data <- dat[train_indices, ]
valid_data <- dat[-train_indices, ]
# Step 2: Prepare the training and validation datasets
X_train <- as.matrix(train_data[, 1:3])  # Input features
Y_train <- as.matrix(train_data[, 4:6])  # Response variables (one-hot encoded)
X_valid <- as.matrix(valid_data[, 1:3])  # Input features
Y_valid <- as.matrix(valid_data[, 4:6])  # Response variables (one-hot encoded)
# Step 3: Define the objective function with regularization
v=0.01
obj_pen <- function(theta)
{
result <- af_forward(X_train, Y_train, theta, m, v)
return(result$obj)
}
# Initial random theta
p <- ncol(X_train)
q <- ncol(Y_train)
m <- 4
npars  <- 2*p^2 + 2*p + 2*p*m + 2*m + m^2 + m*q + q
theta_rand = runif(npars,-1,1)
res_opt = nlm(obj_pen,theta_rand,iterlim = 1000)
# Step 4: Grid search over regularization parameter nu
n_v <- 25
validation_errors = rep(NA,n_v)
v_values <- exp(seq(-10, 1, length.out = n_v))
#validation_errors <- numeric(length(v_values))
for (i in 1:n_v) {
v <- v_values[i]
res_opt = nlm(obj_pen,theta_rand,iterlim = 1000)
res_val = af_forward(X_valid,Y_valid,res_opt$estimate,m,0)
validation_errors[i] = res_val$obj
}
# 1) Compute the medians of X1 and X2 from your data
X1_med <- median(dat$X1)
X2_med <- median(dat$X2)
# 2) Improved helper: fix X1, X2, X3 separately
predict_curve <- function(var_seq, varname,
fixed_X1, fixed_X2, fixed_X3,
pars, m) {
n <- length(var_seq)
input <- matrix(0, n, 3)
colnames(input) <- c("X1","X2","X3")
# Sweep the target variable; hold the others at their medians
input[,"X1"] <- if(varname=="X1") var_seq else fixed_X1
input[,"X2"] <- if(varname=="X2") var_seq else fixed_X2
input[,"X3"] <- fixed_X3
# Forward pass to get probabilities
probs <- af_forward(input,
Y     = matrix(0,n,3),
theta = theta_best,
m     = m,
nu    = 0)$probs
# Return a data.frame with var, class-probs
df <- as.data.frame(probs)
names(df) <- c("alpha","beta","rho")
df[[varname]] <- var_seq
df
}
# 3) Build sequences over the real data range
X1_seq <- seq(min(dat$X1), max(dat$X1), length.out = 200)
X2_seq <- seq(min(dat$X2), max(dat$X2), length.out = 200)
# 4) Generate curves for Detector A (X3=1) vs B (X3=0)
df1_A <- predict_curve(X1_seq, "X1", X1_med, X2_med, 1, theta_best, m)
df1_B <- predict_curve(X1_seq, "X1", X1_med, X2_med, 0, theta_best, m)
df2_A <- predict_curve(X2_seq, "X2", X1_med, X2_med, 1, theta_best, m)
df2_B <- predict_curve(X2_seq, "X2", X1_med, X2_med, 0, theta_best, m)
# 5) Reshape and plot for X1
plot_data_X1 <- bind_rows(
transform(df1_A, Detector="Type A"),
transform(df1_B, Detector="Type B")
) %>%
pivot_longer(cols = c("alpha","beta","rho"),
names_to  = "Class",
values_to = "Probability")
ggplot(plot_data_X1, aes(x = X1, y = Probability, color = Class)) +
geom_line(size=1) +
facet_wrap(~ Detector) +
labs(title = "Response Curves: P(class) vs X1 by Detector",
x = "X1", y = "Predicted Probability") +
theme_minimal() +
theme(aspect.ratio = 1)
# 6) Reshape and plot for X2
plot_data_X2 <- bind_rows(
transform(df2_A, Detector="Type A"),
transform(df2_B, Detector="Type B")
) %>%
pivot_longer(cols = c("alpha","beta","rho"),
names_to  = "Class",
values_to = "Probability")
ggplot(plot_data_X2, aes(x = X2, y = Probability, color = Class)) +
geom_line(size=1) +
facet_wrap(~ Detector) +
labs(title = "Response Curves: P(class) vs X2 by Detector",
x = "X2", y = "Predicted Probability") +
theme_minimal() +
theme(aspect.ratio = 1)
#####new year code-----------
# 1. Read the raw data
raw <- read_excel(
"Peregrine ringing data sightings_1989-2024_13042025.xlsx",
sheet = "Re-sightings - annual",
skip  = 3
)
setwd("~/Documents/GitHub/research-project")
#####new year code-----------
# 1. Read the raw data
raw <- read_excel(
"Peregrine ringing data sightings_1989-2024_13042025.xlsx",
sheet = "Re-sightings - annual",
skip  = 3
)
library(readxl)
library(lubridate)
#####new year code-----------
# 1. Read the raw data
raw <- read_excel(
"Peregrine ringing data sightings_1989-2024_13042025.xlsx",
sheet = "Re-sightings - annual",
skip  = 3
)
# 2. Compute an "Aug–Jul capture‐year" and flag sightings
dat <- raw %>%
rename(
ring      = `ring number`,
date_seen = `date sighted on territory`
) %>%
mutate(
date_seen = as.Date(date_seen),
# if month ≥ 8 (Aug–Dec) → same calendar year; else (Jan–Jul) → previous year
cap_year = if_else(
month(date_seen) >= 9,
year(date_seen),
year(date_seen) - 1L
)
) %>%
select(ring, cap_year) %>%
distinct() %>%
mutate(obs = 1L) %>%
# keep only your 30 seasons 1990–2019
filter(cap_year >= 1990, cap_year <= 2019)
library(dplyr)
library(tidyr)
# 2. Compute an "Aug–Jul capture‐year" and flag sightings
dat <- raw %>%
rename(
ring      = `ring number`,
date_seen = `date sighted on territory`
) %>%
mutate(
date_seen = as.Date(date_seen),
# if month ≥ 8 (Aug–Dec) → same calendar year; else (Jan–Jul) → previous year
cap_year = if_else(
month(date_seen) >= 9,
year(date_seen),
year(date_seen) - 1L
)
) %>%
select(ring, cap_year) %>%
distinct() %>%
mutate(obs = 1L) %>%
# keep only your 30 seasons 1990–2019
filter(cap_year >= 1990, cap_year <= 2019)
# 3. Build every ring × Aug–Jul season from 1990 to 2019
years <- 1990:2019
all_combos <- expand.grid(
ring     = unique(dat$ring),
cap_year = years
)
full <- all_combos %>%
left_join(dat, by = c("ring","cap_year")) %>%
mutate(obs = replace_na(obs, 0L))
# 4. Pivot into your 0/1 encounter‐history (yr1990 … yr2019)
enc_hist <- full %>%
arrange(ring, cap_year) %>%
pivot_wider(
names_from   = cap_year,
values_from  = obs,
names_prefix = "yr",
values_fill  = 0L
)
# 5. (Optional) Get a matrix or .inp file exactly as before:
ch_matrix <- as.matrix(enc_hist[,-1])
rownames(ch_matrix) <- enc_hist$ring
# if you need the .inp format:
caphist_inp <- enc_hist %>%
mutate(caphist = apply(select(., starts_with("yr")), 1, paste0, collapse = "")) %>%
select(caphist) %>%
mutate(freq = 1)
write.table(
caphist_inp,
"peregrine_data_newyear.inp",
quote     = FALSE,
row.names = FALSE,
col.names = FALSE,
sep       = " "
)
View(raw)
#####------------------------
raw <- read_excel(
"Peregrine ringing data sightings_1989-2024_13042025.xlsx",
sheet = "Re-sightings - annual",
skip = 3 # skip metadata rows
)
# 3. Tidy up: rename the ring ID and the year‐sighted columns,
#    and keep just those plus make an “obs” flag = 1
dat <- raw %>%
rename(
ring      = `ring number`,
date_seen = `date sighted on territory`
) %>%
# turn the POSIX date into an integer year
mutate(year_seen = year(date_seen)) %>%
select(ring, year_seen) %>%
distinct() %>%              # one (ring, year) per bird-year
mutate(obs = 1L)            # 1 = seen
# 4. Build a full table of (ring × all years in your study) and join...so even if a bird wasn't seen in a certain year that (ring, year) pair still exists...to be filled with 0s later
years <- 1997:2019
all_combos <- expand.grid(
ring      = unique(dat$ring),
year_seen = years
)
full <- all_combos %>%
left_join(dat, by = c("ring", "year_seen")) %>%
mutate(obs = replace_na(obs, 0L)) # If a bird was not seen in a year, obs is set to 0 using replace_na.
enc_hist <- full %>%
arrange(ring, year_seen) %>%
pivot_wider(
names_from  = year_seen,
values_from = obs,
names_prefix = "yr",
values_fill  = 0L
)
# drop the ring column if you just want the matrix
ch_matrix <- as.matrix(enc_hist[,-1])
rownames(ch_matrix) <- enc_hist$ring
# Or, if you want a 0/1 string:
cap_hist <- enc_hist %>%
mutate(
caphist_str = apply(select(., starts_with("yr")), 1, paste0, collapse = "")
)
# Optional: add frequency column (if each is unique, all freq = 1)
cap_hist_inp <- cap_hist %>%
select(caphist_str) %>%
mutate(freq = 1)
# Write to .inp file (no header, space-separated)
write.table(
cap_hist_inp,
file = "peregrine_data.inp",
quote = FALSE,
row.names = FALSE,
col.names = FALSE,
sep = " "
)
#####new year code-----------
# 1. Read the raw data
raw <- read_excel(
"Peregrine ringing data sightings_1989-2024_13042025.xlsx",
sheet = "Re-sightings - annual",
skip  = 3
)
# 2. Compute an "Aug–Jul capture‐year" and flag sightings
dat <- raw %>%
rename(
ring      = `ring number`,
date_seen = `date sighted on territory`
) %>%
mutate(
date_seen = as.Date(date_seen),
# if month ≥ 8 (Aug–Dec) → same calendar year; else (Jan–Jul) → previous year
cap_year = if_else(
month(date_seen) >= 9,
year(date_seen),
year(date_seen) - 1L
)
) %>%
select(ring, cap_year) %>%
distinct() %>%
mutate(obs = 1L) %>%
# keep only your 30 seasons 1990–2019
filter(cap_year >= 1997, cap_year <= 2019)
# 3. Build every ring × Aug–Jul season from 1990 to 2019
years <- 1997:2019
all_combos <- expand.grid(
ring     = unique(dat$ring),
cap_year = years
)
full <- all_combos %>%
left_join(dat, by = c("ring","cap_year")) %>%
mutate(obs = replace_na(obs, 0L))
# 4. Pivot into your 0/1 encounter‐history (yr1990 … yr2019)
enc_hist <- full %>%
arrange(ring, cap_year) %>%
pivot_wider(
names_from   = cap_year,
values_from  = obs,
names_prefix = "yr",
values_fill  = 0L
)
# 5. (Optional) Get a matrix or .inp file exactly as before:
ch_matrix <- as.matrix(enc_hist[,-1])
rownames(ch_matrix) <- enc_hist$ring
# if you need the .inp format:
caphist_inp <- enc_hist %>%
mutate(caphist = apply(select(., starts_with("yr")), 1, paste0, collapse = "")) %>%
select(caphist) %>%
mutate(freq = 1)
write.table(
caphist_inp,
"peregrine_data_newyear.inp",
quote     = FALSE,
row.names = FALSE,
col.names = FALSE,
sep       = " "
)
